---
title: "proj2-notebook"
author: "Zhanyuan Zhang"
date: "April 15, 2019"
output: html_document
---

```{r}
library(glmnet)
library(ggplot2)
library(dplyr)
library(HDCI)
library(caret)
library(MASS)
library(e1071)
library(glmnet)
library(tidyr)
library(ROCR)
library(randomForest)
data(ROCR.simple)
roc_df <- data.frame(ROCR.simple)
```

```{r}
# Import data
m1 <- read.table("image_data/image1.txt", col.names = c('y', 'x', 'label', 'NDAI', 'SD', 'CORR', 'DF', 'CF', 'BF', 'AF', 'AN'))
m2 <- read.table("image_data/image2.txt", col.names = c('y', 'x', 'label', 'NDAI', 'SD', 'CORR', 'DF', 'CF', 'BF', 'AF', 'AN'))
m3 <- read.table("image_data/image3.txt", col.names = c('y', 'x', 'label', 'NDAI', 'SD', 'CORR', 'DF', 'CF', 'BF', 'AF', 'AN'))
```

# ```{r}
# ggplot(data = m1) +
#   geom_point(aes(x = x, y = y, color = label)) +
#   ggtitle("m1")
# 
# ggplot(data = m2) +
#   geom_point(aes(x = x, y = y, color = label)) +
#   ggtitle("m2")
# 
# ggplot(data = m3) +
#   geom_point(aes(x = x, y = y, color = label)) +
#   ggtitle("m3")
# ```


### 2. 
(a). The spatial dependency is obvious among the pixels. To take the spatial dependency into account, one way of sampling is to divide each image horizontally and sample from the resulting stripes, the other way is to divide each image into equal-size blocks and then sample from these blocks. In this project, we decided to divide each image into blocks and then sample from them, since this is more likely be the presenting way of future data. More specifically, we are going to divide an image into 10*10 blocks and then randomly sample 10 bloks for validation, another 10 blocks for testing, and the rest for training.

(b). Split the data:

### Method 1: Splitting by blocks

```{r}
ggplot() +
  geom_point(data = annotated_m1, aes(x = x, y = y, color = class_label)) +
  ggtitle("Annotated Image 1 (Random seed = 154)") +
  geom_point(data = m1, aes(x = x, y = y), color = m1$label + 21, alpha = 0.01) +
  theme(plot.title = element_text(hjust = 0.5)) +
  labs(color = "class label")

ggplot() +
  geom_point(data = annotated_m2, aes(x = x, y = y, color = class_label)) +
  ggtitle("Annotated Image 2 (Random seed = 189)") +
  geom_point(data = m2, aes(x = x, y = y), color = m2$label + 21, alpha = 0.01) +
  theme(plot.title = element_text(hjust = 0.5)) +
  labs(color = "class label")

ggplot() +
  geom_point(data = annotated_m3, aes(x = x, y = y, color = class_label)) +
  ggtitle("Annotated Image 3 (Random seed = 182)") +
  geom_point(data = m3, aes(x = x, y = y), color = m3$label + 21, alpha = 0.01) +
  theme(plot.title = element_text(hjust = 0.5)) +
  labs(color = "class label")
```

### Method 2: Splitting by blurring the image into larger pixels and then random sampling

```{r}
ggplot(data = blurred_m1) +
  geom_point(aes(x = x, y = y, color = label)) +
  ggtitle("Blurred Image 1") +
  theme(plot.title = element_text(hjust = 0.5)) +
  labs(color = "expert label")
ggplot(data = blurred_m2) +
  geom_point(aes(x = x, y = y, color = label)) +
  ggtitle("Blurred Image 2") +
  theme(plot.title = element_text(hjust = 0.5)) +
  labs(color = "expert label")
ggplot(data = blurred_m3) +
  geom_point(aes(x = x, y = y, color = label)) +
  ggtitle("Blurred Image 3") +
  theme(plot.title = element_text(hjust = 0.5)) +
  labs(color = "expert label")
```

```{r}
val_data <- rbind.data.frame(m1_val, m2_val, m3_val)
test_data <- rbind.data.frame(m1_test, m2_test, m3_test)
train_data <- rbind.data.frame(m1_train, m2_train, m3_train)
```

Data cleaning for blocking:
```{r}
bin_train <- train %>% filter(label != 0)
bin_val <- val %>% filter(label != 0)
bin_test <- test %>% filter(label != 0)

train_label <- ifelse(bin_train$label == 1, 1, 0)
val_label <- ifelse(bin_val$label == 1, 1, 0)
test_label <- ifelse(bin_test$label == 1, 1, 0)

bin_train <- bin_train %>% mutate(label = train_label)
bin_val <- bin_val %>% mutate(label = val_label)
bin_test <- bin_test %>% mutate(label = test_label)

bin_train <- bin_train[, -c(1, 2)]
bin_val <- bin_val[, -c(1, 2)]
bin_test <- bin_test[, -c(1, 2)]

# Standardize
std_train <- data.frame(scale(bin_train)) %>% mutate(label = bin_train$label)

std_val <- data.frame(scale(bin_val)) %>% mutate(label = bin_val$label)

std_test <- data.frame(scale(bin_test)) %>% mutate(label = bin_test$label)
```

Data cleaning for blurring:
```{r}
set.seed(154)
blur_concat <- rbind.data.frame(blurred_m1, blurred_m2, blurred_m3)
blur_concat <- blur_concat[, -12]
val_test_indices <- sample(1:nrow(blur_concat), round(nrow(blur_concat)*0.2), replace = FALSE)
val_indices <- val_test_indices[1:round(length(val_test_indices)*0.5)]
test_indices <- val_test_indices[(round(length(val_test_indices)*0.5) + 1):length(val_test_indices)]
blur_val <- blur_concat[val_indices, ]
blur_test <- blur_concat[test_indices, ]
blur_train <- blur_concat[-c(val_indices, test_indices), ]

bin_blur_val <- blur_val[, -c(1, 2)] %>% filter(label != 0)
bin_blur_val <- bin_blur_val %>% mutate(label = ifelse(bin_blur_val$label == 1, 1, 0))

bin_blur_test <- blur_test[, -c(1, 2)] %>% filter(label != 0)
bin_blur_test <- bin_blur_test %>% mutate(label = ifelse(bin_blur_test$label == 1, 1, 0))

bin_blur_train <- blur_train[, -c(1, 2)] %>% filter(label != 0)
bin_blur_train <- bin_blur_train %>% mutate(label = ifelse(bin_blur_train$label == 1, 1, 0))

std_blur_val <- data.frame(scale(bin_blur_val)) %>% mutate(label = bin_blur_val$label)
std_blur_test <- data.frame(scale(bin_blur_test)) %>% mutate(label = bin_blur_test$label)
std_blur_train <- data.frame(scale(bin_blur_train)) %>% mutate(label = bin_blur_train$label)
```

# Baseline - Trivial classifier
```{r}
print(nrow(std_val %>% filter(label == 0)) / nrow(std_val))
print(nrow(std_test %>% filter(label == 0)) / nrow(std_test))
```

# First Order Importance
```{r}
data.frame(std_train[, -1]) %>% 
  dplyr::select(NDAI:CORR) %>%
  gather(Variable, Value) %>% 
  ggplot(aes(x=Value, fill=Variable)) +
      geom_density(alpha=0.5) +
      theme_bw() +
      scale_fill_brewer(palette="Spectral")
```

```{r}
data.frame(std_train[, -1]) %>% 
  dplyr::select(DF:AN) %>%
  gather(Variable, Value) %>% 
  ggplot(aes(x=Value, fill=Variable)) +
      geom_density(alpha=0.5) +
      theme_bw() +
      scale_fill_brewer(palette="Spectral")
```

```{r}
library(dplyr)

cor = abs(cor(bin_train$label, bin_train[, -1]))
names = colnames(cor)
cor = data.frame(as.numeric(cor), names)
colnames(cor) = c("correlation" , "features")
ggplot(cor, aes(x = reorder(features, -correlation), y = correlation)) +
  geom_bar(stat = "identity") +
  xlab("features") +
  ggtitle("Absolute Correlation between Label and Features in Training Set") +
  theme(plot.title = element_text(hjust = 0.5))

ggplot(bin_train) +
  geom_histogram(aes(x = NDAI, fill = factor(label)), alpha = 0.7) +
  labs(fill = "expert label") +
  geom_vline(xintercept = 0.215)

ggplot(bin_train) +
  geom_histogram(aes(x = CORR, fill = factor(label)), alpha = 0.7) +
  labs(fill = "expert label") +
  geom_vline(xintercept = 0.21)

ggplot(bin_train) +
  geom_histogram(aes(x = log(SD+1), fill = factor(label)), alpha = 0.7) +
  labs(fill = "expert label") +
  geom_vline(xintercept = 1.4)

std_train0 = filter(std_train, label == 0)
std_train1 = filter(std_train, label == 1)
mean_difference = c()
for (i in 2:9) {
  abs_diff = abs(mean(std_train0[,i]) - mean(std_train1[,1]))
  mean_difference = c(mean_difference, abs_diff)
}
mean_difference = data.frame(c('NDAI', 'SD', 'CORR', 'DF', 'CF', 'BF', 'AF', 'AN'), mean_difference)
colnames(mean_difference) = c("Feature", "Absolute Mean Difference (Standardized)")
mean_difference = arrange(mean_difference, desc(`Absolute Mean Difference (Standardized)`))
mean_difference
```

```{r}
get_logit_pred <- function(clf, data, thresh=0.5) {
  ifelse(1 / (1 + exp(-predict(clf, data))) >= thresh, 1, 0)
}

CVgeneric <- function(train_x, train_y, test_x, test_y, k, loss_fn, model, thresh=0.5) {
  flds = createFolds(1:nrow(train_x), k = k)
  avg_acc = c()
  test_acc = c()
  for (i in 1:k) {
    cv_train = data.frame()
    for (j in 1:(k-1)) {
      cv_train = rbind.data.frame(cv_train, train_x[flds[[j]], ])
    }
    cv_val = train_x[flds[[i]], ]
    cv_val_label = train_y[flds[[i]]]
    if (model == 'lda') {
      clf = lda(label ~ ., data = cv_train)
    } else if (model == 'qda') {
      clf = qda(label ~ ., data = cv_train)
    } else if (model == 'logit') {
      clf = glm(label ~ ., data = cv_train, family = binomial(link = "logit"))
      y_hat = get_logit_pred(clf, cv_val, thresh = thresh)
      temp = loss_fn(cv_val_label, y_hat)
      test_pred = get_logit_pred(clf, test_x, thresh = thresh)
      test_temp = loss_fn(test_y, test_pred)
      avg_acc = c(avg_acc, temp)
      test_acc = c(test_acc, test_temp)
    }
    if (model != "logit") {
      y_hat = predict(clf, cv_val)$class
      temp = loss_fn(cv_val_label, y_hat)
      
      test_hat = predict(clf, test_x)$class
      test_temp = loss_fn(test_y, test_hat)
      
      avg_acc = c(avg_acc, temp)
      test_acc = c(test_acc, test_temp)
    }
  }
  return(list(cv_avg_acc = avg_acc, cv_test_acc = test_acc))
}

loss_fn <- function(train_y, y_hat) return(sum(train_y == y_hat) / length(y_hat))
```


# Model

## CV for Method 1

## LDA (non-standardized data)
```{r}
CVgeneric(rbind.data.frame(bin_train, bin_val), c(bin_train$label, bin_val$label), 10, loss_fn, 'lda')
```

```{r}
CVgeneric(rbind.data.frame(bin_blur_train, bin_blur_val), c(bin_blur_train$label, bin_blur_val$label), bin_blur_test, bin_blur_test$label, 10, loss_fn, 'lda')
```

## QDA (non-standardized data)
```{r}
CVgeneric(rbind.data.frame(bin_train, bin_val), c(bin_train$label, bin_val$label), 10, loss_fn, 'qda')
```

## Logistic Regression (non-standardized data)
```{r}
CVgeneric(rbind.data.frame(bin_train, bin_val), c(bin_train$label, bin_val$label), 10, loss_fn, 'logit')
```

## LDA (standardized data)
```{r}
CVgeneric(rbind.data.frame(std_train, std_val), c(std_train$label, std_val$label), 10, loss_fn, 'lda')
```

## QDA (standardized data)
```{r}
CVgeneric(rbind.data.frame(std_train, std_val), c(std_train$label, std_val$label), 10, loss_fn, 'qda')
```

## Logistic Regression (standardized data)
```{r}
#for (thresh in seq(0.45, 1, 0.01)) {
#  CVgeneric(rbind.data.frame(std_train, std_val), c(std_train$label, std_val$label), 10, loss_fn, 'logit', thresh)
#}
cv_scores <- c()
for (thresh in c(0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75)) {
  print(thresh)
  cv_list <- CVgeneric(rbind.data.frame(std_train, std_val), c(std_train$label, std_val$label), 10, loss_fn, 'logit', thresh)
  print(cv_list)
  cv_scores <- c(cv_scores, mean(cv_list))
  print(cv_scores)
  print("**********")
}
#CVgeneric(rbind.data.frame(std_train, std_val), c(std_train$label, std_val$label), 10, loss_fn, 'logit', 0.5)
cv_scores
```


## Random Forest (standardized data)
```{r}
# 10, 20 <- 0.912
forest.acc <- c()
for (n in 10:20) {
  for (i in 5:20) {
    print(c(n, i))
    forest.model <- randomForest(factor(label) ~., data = std_train, maxnodes=n, ntree = i)
    temp <- sum(predict(forest.model, std_val) == std_val$label) / nrow(std_val)
    print(temp)
    forest.acc <- c(forest.acc, temp)
  }
}
#forest.model <- randomForest(factor(label) ~., data = std_train, maxnodes=10, ntree = 20)
max(forest.acc)
```

```{r}
sum(predict(forest.model, std_val) == std_val$label) / nrow(std_val)
```

```{r}
forest.model$confusion
```

## CV for Method 2

## LDA (non-standardized data)
```{r}
CVgeneric(rbind.data.frame(bin_blur_train, bin_blur_val), c(bin_blur_train$label, bin_blur_val$label), 10, loss_fn, 'lda')
```

## QDA (non-standardized data)
```{r}
CVgeneric(rbind.data.frame(bin_blur_train, bin_blur_val), c(bin_blur_train$label, bin_blur_val$label), 10, loss_fn, 'qda')
```

## Logistic Regression (non-standardized data)
```{r}
CVgeneric(rbind.data.frame(bin_blur_train, bin_blur_val), c(bin_blur_train$label, bin_blur_val$label), 10, loss_fn, 'logit')
```

## LDA (standardized data)
```{r}
CVgeneric(rbind.data.frame(std_blur_train, std_blur_val), c(std_blur_train$label, std_blur_val$label), 10, loss_fn, 'lda')
```

## QDA (standardized data)
```{r}
CVgeneric(rbind.data.frame(std_blur_train, std_blur_val), c(std_blur_train$label, std_blur_val$label), 10, loss_fn, 'qda')
```

## Logistic Regression (standardized data)
```{r}
#for (thresh in seq(0.45, 1, 0.01)) {
#  CVgeneric(rbind.data.frame(std_train, std_val), c(std_train$label, std_val$label), 10, loss_fn, 'logit', thresh)
#}

cv_scores <- c()
for (thresh in c(0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75)) {
  print(thresh)
  cv_list <- CVgeneric(rbind.data.frame(std_blur_train, std_blur_val), c(std_blur_train$label, std_blur_val$label), 10, loss_fn, 'logit', thresh)
  print(cv_list)
  cv_scores <- c(cv_scores, mean(cv_list))
  print(cv_scores)
  print("**********")
}
which.max(cv_scores)

#CVgeneric(rbind.data.frame(std_blur_train, std_blur_val), c(std_blur_train$label, std_blur_val$label), 10, loss_fn, 'logit', 0.4)
```

## Random Forest (standardized data)
```{r}
# 10, 20 <- 0.912
forest.acc <- c()
for (n in 10:20) {
  for (i in 5:20) {
    print(c(n, i))
    forest.model <- randomForest(factor(label) ~., data = std_blur_train, maxnodes=n, ntree = i)
    temp <- sum(predict(forest.model, std_blur_val) == std_blur_val$label) / nrow(std_blur_val)
    print(temp)
    forest.acc <- c(forest.acc, temp)
  }
}
#forest.model <- randomForest(factor(label) ~., data = std_train, maxnodes=10, ntree = 20)
max(forest.acc)
```

```{r}
sum(predict(forest.model, std_blur_val) == std_blur_val$label) / nrow(std_blur_val)
```

```{r}
forest.model$confusion
```


# ROC curve for Parameter Tuning

## LDA
```{r}
lda.model <- lda(label ~., data = std_train)
lda.pred <- predict(lda.model, std_val)
pred = prediction(lda.pred$posterior[, 2], std_val$label)
perf <- performance(pred, "tpr", "fpr")
plot(perf, colorize=TRUE)
points(x = 1-0.8842453, y = 0.9055535, pch = 15, col = "red")
abline(h = 0.9055535, v = 1-0.8842453, col = "grey", lty = 2)
text(x = 0.25, y = 0.82, labels = "Cutoff Value = 0.28")
title('ROC curve of LDA')
```

```{r}
opt.cut = function(perf, pred){
    cut.ind = mapply(FUN=function(x, y, p){
        d = (x - 0)^2 + (y-1)^2
        ind = which(d == min(d))
        c(sensitivity = y[[ind]], specificity = 1-x[[ind]], 
            cutoff = p[[ind]])
    }, perf@x.values, perf@y.values, pred@cutoffs)
}

print(opt.cut(perf, pred))
```


## QDA
```{r}
qda.model <- qda(label ~., data = std_train)
qda.pred <- predict(qda.model, std_val)
pred = prediction(qda.pred$posterior[, 2], std_val$label)
perf <- performance(pred, "tpr", "fpr")
plot(perf, colorize=TRUE)
points(x = 1-0.89168671, y = 0.95768795, pch = 15, col = "red")
abline(h = 0.95768795, v = 1-0.89168671, col = "grey", lty = 2)
text(x = 0.25, y = 0.87, labels = "Cutoff Value = 0.03")
title('ROC curve of QDA')
```

```{r}
opt.cut = function(perf, pred){
    cut.ind = mapply(FUN=function(x, y, p){
        d = (x - 0)^2 + (y-1)^2
        ind = which(d == min(d))
        c(sensitivity = y[[ind]], specificity = 1-x[[ind]], 
            cutoff = p[[ind]])
    }, perf@x.values, perf@y.values, pred@cutoffs)
}

print(opt.cut(perf, pred))
```


## Logitsitc regression
```{r}
logit.model <- glm(label ~ ., data = std_train, family = binomial(link = "logit"))
logit_pred <- 1/(1 + exp(-predict(logit.model, std_val)))
pred = prediction(logit_pred, std_val$label)
perf <- performance(pred,"tpr","fpr")
plot(perf,colorize=TRUE)
points(x = 1-0.8842453, y = 0.9055535, pch = 15, col = "red")
abline(h = 0.9055535, v = 1-0.8842453, col = "grey", lty = 2)
text(x = 0.25, y = 0.82, labels = "Cutoff Value = 0.28")
title('ROC curve of logistic regression')
```

```{r}
opt.cut = function(perf, pred){
    cut.ind = mapply(FUN=function(x, y, p){
        d = (x - 0)^2 + (y-1)^2
        ind = which(d == min(d))
        c(sensitivity = y[[ind]], specificity = 1-x[[ind]], 
            cutoff = p[[ind]])
    }, perf@x.values, perf@y.values, pred@cutoffs)
}

print(opt.cut(perf, pred))
```

## Random Forest
```{r}
rf.model <- randomForest(factor(label) ~., data = rbind.data.frame(std_train, std_val), maxnodes=100, ntree=10)
rf.pred <- as.data.frame(predict(rf.model, std_test, type="prob"))
pred <- prediction(rf.pred$`1`, std_test$label)
perf <- performance(pred, "tpr", "fpr")
plot(perf, colorize=TRUE)
title('ROC curve of random forest')
```


# ROC Curve Comparing Models

```{r}
std_combined = rbind(std_train, std_val)
```


## LDA
```{r}
lda.model <- lda(label ~., data = std_combined)
lda.pred <- predict(lda.model, std_test)
pred = prediction(lda.pred$posterior[, 2], std_test$label)
perf <- performance(pred, "tpr", "fpr")
plot(perf, colorize=TRUE)
points(x = 1-0.8361833, y = 0.9300521, pch = 15, col = "red")
abline(h = 0.9300521, v = 1-0.8361833, col = "grey", lty = 2)
text(x = 0.3, y = 0.82, labels = "Cutoff Value = ?")
title('ROC curve of LDA')
```

```{r}
opt.cut = function(perf, pred){
    cut.ind = mapply(FUN=function(x, y, p){
        d = (x - 0)^2 + (y-1)^2
        ind = which(d == min(d))
        c(sensitivity = y[[ind]], specificity = 1-x[[ind]], 
            cutoff = p[[ind]])
    }, perf@x.values, perf@y.values, pred@cutoffs)
}

print(opt.cut(perf, pred))
```


## QDA
```{r}
qda.model <- qda(label ~., data = std_combined)
qda.pred <- predict(qda.model, std_test)
pred = prediction(qda.pred$posterior[, 2], std_test$label)
perf <- performance(pred, "tpr", "fpr")
plot(perf, colorize=TRUE)
points(x = 1-0.86159660, y = 0.92883986, pch = 15, col = "red")
abline(h = 0.92883986, v = 1-0.86159660, col = "grey", lty = 2)
text(x = 0.25, y = 0.87, labels = "Cutoff Value = 0.02")
title('ROC curve of QDA')
```

```{r}
opt.cut = function(perf, pred){
    cut.ind = mapply(FUN=function(x, y, p){
        d = (x - 0)^2 + (y-1)^2
        ind = which(d == min(d))
        c(sensitivity = y[[ind]], specificity = 1-x[[ind]], 
            cutoff = p[[ind]])
    }, perf@x.values, perf@y.values, pred@cutoffs)
}

print(opt.cut(perf, pred))
```


## Logitsitc regression
```{r}
logit.model <- glm(label ~ ., data = std_combined, family = binomial(link = "logit"))
logit_pred <- 1/(1 + exp(-predict(logit.model, std_test)))
pred = prediction(logit_pred, std_test$label)
perf <- performance(pred,"tpr","fpr")
plot(perf,colorize=TRUE)
points(x = 1-0.8350496, y = 0.9329616, pch = 15, col = "red")
abline(h = 0.9329616, v = 1-0.8350496, col = "grey", lty = 2)
text(x = 0.25, y = 0.82, labels = "Cutoff Value = 0.26")
title('ROC curve of logistic regression')
```

```{r}
opt.cut = function(perf, pred){
    cut.ind = mapply(FUN=function(x, y, p){
        d = (x - 0)^2 + (y-1)^2
        ind = which(d == min(d))
        c(sensitivity = y[[ind]], specificity = 1-x[[ind]], 
            cutoff = p[[ind]])
    }, perf@x.values, perf@y.values, pred@cutoffs)
}

print(opt.cut(perf, pred))
```

# ```{r}
# knn.acc <- c()
# for (k in 1:10) {
#   knn.pred <- class::knn(std_train[, -1], std_val[, -1], std_train$label, k = k)
#   knn.acc <- c(knn.acc, sum(knn.pred == std_val$label) / nrow(std_val))
# }
# knn.acc
# ```
# 
# ```{r}
# acc <- c()
# for (k in seq(0.01, 1, 0.01)) {
#   acc <- c(acc, sum(ifelse(logit_pred >= k, 1, 0) == std_val) / nrow(std_val))
# }
# 
# plot(x=seq(0.01, 1, 0.01), acc, type='l')
# ```


# Other Metrics
```{r}
confusionMatrix(lda.pred$class, factor(std_test$label)) 
confusionMatrix(qda.pred$class, factor(std_test$label)) 
logit_pred <- get_logit_pred(logit.model, std_test)
confusionMatrix(factor(logit_pred), factor(std_test$label)) 
```

AIC
```{r}
library(mvtnorm)

input_labels = std_train$label
mean_0 = colMeans(filter(std_train[, -1], input_labels == 0))
mean_1 = colMeans(filter(std_train[, -1], input_labels == 1))
mean = list(mean_0, mean_1)

lam = 1e-5
cov_0 = cov(filter(std_train[, -1], input_labels == 0)) + lam * diag(nrow = 8) 
cov_1 = cov(filter(std_train[, -1], input_labels == 1)) + lam * diag(nrow = 8)
cov = list(cov_0, cov_1)
n0 = nrow(filter(std_train[, -1], input_labels == 0))
n1 = nrow(filter(std_train[, -1], input_labels == 1))
cov_lda = ((n0-1)*cov_0 + (n1-1)*cov_1)/(n0-1+n1-1)

# QDA
input_0 = filter(std_train, std_train$label == 0)
input_0 = input_0[,-1]
loglik_0 = sum(dmvnorm(input_0, mean = as.numeric(unlist(mean[1])), sigma = matrix(unlist(cov[1]), nrow = 8), log = T))

input_1 = filter(std_train, std_train$label == 1)
input_1 = input_0[,-1]
loglik_1 = sum(dmvnorm(input_0, mean = as.numeric(unlist(mean[2])), sigma = matrix(unlist(cov[2]), nrow = 8), log = T))

loglik_qda = loglik_0 + loglik_1

k = 2*(8*8/2+8)
AIC_qda = 2 * k - 2 * loglik_qda

# LDA
input_0 = filter(std_train, std_train$label == 0)
input_0 = input_0[,-1]
loglik_0 = sum(dmvnorm(input_0, mean = as.numeric(unlist(mean[1])), sigma = matrix(unlist(cov_lda), nrow = 8), log = T))

input_1 = filter(std_train, std_train$label == 1)
input_1 = input_0[,-1]
loglik_1 = sum(dmvnorm(input_0, mean = as.numeric(unlist(mean[2])), sigma = matrix(unlist(cov_lda), nrow = 8), log = T))

loglik_lda = loglik_0 + loglik_1
AIC_lda = 2 * k - 2 * loglik_lda

#Logistic
AIC_logistic = logit.model$aic
```

BIC
```{r}
n = as.numeric(count(std_train)[1])
BIC_qda = log(n)* k - 2 * loglik_qda
BIC_lda = log(n)* k - 2 * loglik_lda
BIC(logit.model)
```


